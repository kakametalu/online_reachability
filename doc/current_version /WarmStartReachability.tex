\documentclass{journal}
\pdfoutput=1

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{mathabx}
\usepackage[ruled,vlined,titlenotnumbered]{algorithm2e} 
\usepackage{graphicx} 
\usepackage{subcaption}
\usepackage{epsfig} 
\usepackage{cancel}
\usepackage{amssymb}
\usepackage{color}
\usepackage{resizegather} 
\usepackage{amssymb}


\usepackage{todonotes}
\newcommand{\smalltodo}{\todo[size=\footnotesize]}
\newcommand{\linetodo}{\todo[inline]}
\setlength{\marginparwidth}{1.3cm}


\usepackage{ifthen}
\newboolean{include-notes}
\setboolean{include-notes}{true}
 \newcommand{\jfnote}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{blue}{\textbf{[JF: #1]}}}{}}

\usepackage{enumerate}

\usepackage{url}
\usepackage{hyperref}
\usepackage[%
    style=numeric-comp,
    sorting=nyt,
    backend=bibtex,
    sortcites=true,
    doi=false,
    firstinits=true,
    hyperref,
    isbn=false,
    eprint=false,
    maxcitenames=3,
    block=none]
    {biblatex}
    
    \renewbibmacro{in:}{}
    \AtEveryBibitem{
  	\clearlist{language}
	\clearfield{pages}
	}
      
\DeclareBibliographyCategory{needsurl}
\newcommand{\entryneedsurl}[1]{\addtocategory{needsurl}{#1}}
\renewbibmacro*{url+urldate}{%
  \ifcategory{needsurl}{
    \printfield{url}%
    \iffieldundef{urlyear}
      {}
      {\setunit*{\addspace}%
       \printurldate}}
    {}}


\bibliography{library}
\entryneedsurl{Mitchell2004}




\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\K}{\mathcal{K}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\O}{\mathcal{O}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}


\renewcommand{\AA}{\mathbb{A}}
\newcommand{\BB}{\mathbb{B}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\DD}{\mathbb{D}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\JJ}{\mathbb{J}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\MM}{\mathbb{M}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\OO}{\mathbb{O}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\renewcommand{\SS}{\mathbb{S}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\UU}{\mathbb{U}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\WW}{\mathbb{W}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\YY}{\mathbb{Y}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\fixwidth}[1]{\resizebox{\columnwidth}{!}{\ensuremath{\displaystyle{#1}}}} 


\newcommand{\Dx}{\hat\D(x)}
\newcommand{\XXD}{\XX_{\hat{\D}}}
\newcommand{\bu}{\bm{u}}
\newcommand{\bdelta}{\bm{d}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\bx}{\xi}

\newcommand{\Disc}{\text{Disc}}
\DeclareMathOperator{\interior}{int}

\newcommand{\optud}{\underset{u\in\U}{\max}\text{ }\underset{ d\in\D}{\min}}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}


\newcommand{\Vsdr}{V_2}



\title{\LARGE \bf
Fast Computation of Parameterized Reachable Sets via Learning 
}
\author{
Anayo K. Akametalu\textsuperscript{$*$} \and Shromona Ghosh \and Jaime F. Fisac \and Claire J. Tomlin
\thanks{
 Department of Electrical Engineering and Computer Sciences, 
        University of California, Berkeley , United States.\newline
        {\tt\small \{kakametalu, shromona.ghosh, tomlin\}~@eecs.berkeley.edu }}%  
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%\begin{abstract}
%Reachability analysis has proven to be a useful tool when designing controllers for constraint satisfaction in systems that can be modeled. This is typically done by finding an invariant set of constraint-satisfying states and policy, such that it is guaranteed the system never exits this invariant set. Models in control often contain parameters that capture physical properties of the system (e.g. mass, moment of intertia) being studied or some intentionality of an agent (e.g. parameterized policy), thus the results obtained from reachability analysis depend on the values of the parameters. Unfortunately, the solutions are often not analytic and must be computed numerically for any given parameter values. In this paper we leverage computations for previously given parameter values to speed-up the analysis for new values of the parameters. This work is relevant if one wants to construct a library of reachable sets indexed by parameter values, if the parameters in our model are slowly time-varying, or if the parameters are unknown and thus are constantly estimated.
%\end{abstract}


\section{Introduction}
Reachability analysis has proven to be a useful tool when designing controllers for constraint satisfaction in systems that can be modeled. This is typically done by finding an invariant set of constraint-satisfying states and policy, such that it is guaranteed the system never exits this invariant set, which in this paper will be referred to as the safe set. The models studied in control typically belong to a parameterized class, where the parameters capture physical properties of the system (e.g. mass, moment of intertia) being studied or some intentionality of an agent (e.g. weights in a cost function), thus the results obtained from reachability analysis depend on the values of the parameters. Unfortunately, the solutions are often not analytic and must be computed numerically for given parameter values.

Due to the computational overhead safe sets are computed offline. This is a sufficient approach assuming that the model being used does not change. However, in practice a model is used to estimate a system, and as more observations are made from the system the model is subject to change. This also means that the safe set should be modified to reflect this change. In prior work we presented a way to recompute the safe set as the model changes, and in other work we presented a model-free method to modify the safe set locally using just observations. The model-based approach was nearly impractical, as it involved computing safe sets online. The model-free approach is very cheap computationally, but is restricted to making local updates. In addition, since we are maintaining a model of the system, it makes sense to leverage the model to make global updates to the safe set.  

In this paper we develop an efficient way to compute safe sets online. The insight behind this approach is that often times as the model parameters vary the safe set does not change significantly, so given a safe set computed for a parameter value, if the parameter changes slightly the new approach can utilize the older safe set to more quickly compute the new safe set. 

\subsection*{Notation}

Throughout the paper, we use boldface symbols to denote time signals or trajectories.
For any nonempty set $\M\subset\RR^m$, $s_{\M}:\RR^m\to\RR$ denotes the \emph{signed distance function} to $\M$
\[s_\M(z) := \begin{cases}\inf_{y\in\M} |z-y|,&z\in\RR^m\setminus\M, \\ -\inf_{y\in\RR^m\setminus\M} |z-y|,&z\in\M,\end{cases}\]
where $|\cdot|$ denotes a norm on $\RR^m$.

\section{Background \label{sec:back}} \
The constraint-satisfaction problem can be posed as a differential game in which we aim to find the infinite-horizon reachable set of a particular target. In this section we highlight the theory and implementation traditionally used to solve this problem.

% !TEX root = SafeLearning.tex
\subsection{System Model \label{subsec:dynamics}}

The analysis in this paper considers a fully observable system whose underlying dynamics may be non-deterministic, but bounded. 
We can formalize this as a dynamical system with state $x\in\RR^n$, and two inputs, $u\in\U\subset\RR^{n_u},  d\in\D\subset\RR^{n_d}$
(with $\U$ and $\D$ compact)
which we will refer to as the \emph{controller} and the \emph{disturbance}:
\begin{equation}\label{fxud}
\dot{x} = f(x,u, d).
\end{equation}
In this context, however, 
$d$ is thought of as a disturbance capturing the non-deterministic or unknown portions of the dynamics, and the flow field $f: \RR^n \times \U \times \D\rightarrow\RR^n$ is assumed to be uniformly continuous and bounded. Later we will consider working with a parameterized class of flow fields.

Letting $\UU $ and $\DD$ denote the collections of measurable%
	\footnote{A function $f:X\to Y$ between two measurable spaces $(X,\Sigma_X)$ and $(Y,\Sigma_Y)$
	is said to be measurable if the preimage of a measurable set in $Y$ is a measurable set in $X$, that is:
	$\forall V\in\Sigma_Y, f^{-1}(V)\in\Sigma_X$, with $\Sigma_X,\Sigma_Y$ $\sigma$-algebras on $X$,$Y$.}
functions $\bm u: [0,\infty)\to \U $ and $\bm d: [0,\infty)\to \D$ respectively,
and allowing the controller and disturbance to choose any such signals,
the evolution of the system
from any initial state $x$
is determined (see for example \cite{Coddington1955}, Ch. 2, Theorems 1.1, 2.1) by the unique continuous trajectory $\bx:[0,\infty)\to\RR^n$ solving
\begin{equation}\label{eq:xdot}
\begin{split}
\dot{\bx}(s) &= f(\bx(s),\bu(s),\bdelta(s)), \text{ a.e. }s\ge 0,\\
\bx(0) &= x.
\end{split}
\end{equation}
Note that this is a solution in Carath\'eodory's \emph{extended sense}, that is, it satisfies the differential equation \emph{almost everywhere} (i.e. except on a subset of Lebesgue measure zero).


Throughout our analysis, we will use the notation $\bx_{x}^{\bu,\bdelta}(\cdot)$ to denote the state trajectory $t\mapsto x$ corresponding to the initial condition $x\in\RR^n$, the control signal $\bu\in\UU$ and the disturbance signal $\bdelta\in\DD$.



% !TEX root = SafeLearning.tex
\subsection{Safety as a Differential Game}\label{subsec:formulation_constraints}
In the context of this work, safety is represented as a infinite-horizon state-constraint problem, i.e. the system must remain in some region of the state space for all time. Such problems can be posed as optimal control problems when the dynamics are known and deterministic. When the dynamics are non-deterministic a differential game is required. Here we address this more general case. 

\subsubsection{State-Constraint}
A central element in our problem is the \emph{state-constraint set}, which defines a region $\K\subseteq \RR^n$ of the state space, typically resulting from safety considerations, where the system is required to remain at all times. For technical purposes detailed below, we assume that this set is closed; no further assumptions (boundedness, connectedness, convexity, etc.) are made in the analysis. Our problem can thus be stated as finding the \emph{safe set} $\Omega(\K)$ , the set of states $x$ from which our system can start and we can guarantee that a control signal exists to keep the state trajectory within $\K$ irrespective of the disturbance signal. It will be useful to introduce the target set, $\T=\overline{\K}$, which is the complement of the constraint. Note that throughout the document the target set may be referred to as the keep-out set or avoid set. 

\subsubsection{Minimum distance to target}
The target set can be implicitly characterized as the sub-zero level set of a Lipschitz \emph{surface function} $l:\RR^n\rightarrow\RR$: 
\begin{equation}\label{eq:l}
x\in\T\iff l(x)<0.
\end{equation}
This function always exists, since we can simply choose the function $l(x) = s_{\T}(x)$, 
which is Lipschitz continuous by construction. 

To express whether a given trajectory \emph{ever} violates the constraints, let the functional $\V:\RR^n\times\UU\times\DD\to\RR$ assign to each initial state $x$ and input signals $\bu(\cdot)$, $\bdelta(\cdot)$ the lowest value of $l(\cdot)$ achieved by trajectory $\bx_{x,\hat\D}^{\bu}(\cdot)$ over all times $t\ge0$: 
\begin{equation}\label{eq:min_dist_functional}
\mathcal{V}\big(x,\bu(\cdot),\bdelta(\cdot)\big) := \inf_{t\ge 0}l\big(\bx_{x}^{\bu,\bdelta}(t)\big).
\end{equation}
This outcome $\V$ will be strictly smaller than zero if there exists any $t\in[0,\infty)$ at which the trajectory leaves the constraint set, and will be nonnegative if the system remains in the constraint set for all of $t\ge 0$. Denoting $\V^{\bu,\bdelta}(x) = \V\big(x,\bu(\cdot),\bdelta(\cdot)\big)$, the following statement follows from \eqref{eq:l} and \eqref{eq:min_dist_functional} by construction. 

Guaranteeing safe evolution from a given point $x\in\RR^n$ requires determining whether there exists a control input $\bu(\cdot)\in\UU$ such that, for all disturbance inputs $\bdelta(\cdot)\in\DD$ satisfying $\bdelta(t)\in\D$, the evolution of the system remains in $\K$, or equivalently $\V^{\bu,\bdelta}(x)\ge 0$.

Our safe set can then be obtained by solving a differential game (or optimal control problem in the case where there is no disturbance). The game is played between the control and disturbance signal with the restriction that the disturbance signal can  only use  \emph{nonanticipative strategies}. The set of nonanticipative strategies for the disturbance is $\B = \{\bbeta:\UU\to\DD\;|\;
\forall t\ge 0,\; \forall \bu(\cdot),\hat{\bu}(\cdot)\in\UU,$
${\big(\bu(\tau) \!=\! \hat{\bu}(\tau)\text{ a.e.} \tau\ge0\big)\Rightarrow
{\big(\bbeta[\bu](\tau) \!=\! \bbeta[\hat{\bu}](\tau)}{\text{ a.e.} \tau\ge0\big)}}\}$. With this in place we can define the value function $V(x)$ and ultimately the safe set. 

\begin{equation}
V(x)=\inf_{\beta[\bu](\cdot) \in \B} \sup_{\bu \in \UU}\mathcal{V}\big(x,\bu(\cdot),\bdelta(\cdot)\big) .
\end{equation}

The safe set can thus be characterized as 
\begin{equation} \label{eq:safe_set}
\Omega(\K) = \{x \mid V(x) \ge 0\}.
\end{equation}  

It's known that the value function can be obtained from the \emph{finite-horizon value function} $V(x,t)$ which is the unique viscosity solution for a Hamilton Jacobi variational inequality.\footnote{$V(x,t)$ characterizes the set of initial states from which the system can be kept inside $\K$ for a horizon of $t$.
} 
\begin{subequations}\label{eq:HJI}\begin{align}
    & 0 = \min\left\{l(x)-V(x,t), \frac{\partial V}{\partial t}(x,t)+ \max_{u\in\U} \min_{ d\in\D} \!\!\frac{\partial V}{\partial x}(x,t) f(x,u, d)\right\}\label{eq:HJIa}\\
    &V(x,0) = l(x).\label{eq:HJIb}\\
    &V(x) = \lim_{t \rightarrow \infty} V(x,t)
\end{align}\end{subequations}




\subsubsection{Computing Value Function}

The variational inequality (\ref{eq:HJI}) is typically solved using an approximation scheme on a fixed grid $G$. Here we will use a semi-Lagrangian approximation based on a Discrete Time Dynamic Programming Principle, which yields
\begin{subequations}
\begin{align}
&V_{\Delta t}^{k+1}(x) = \min\{l(x),  \underset{u\in\U}{\max}\text{ }\underset{ d\in\D}{\min} V^{k}_{\Delta t}(x+\Delta t f(x,u,d))\}\\
&V_{\Delta t}^{0}(x) = l(x)\\
&V_{\Delta t} = \lim_{k\rightarrow \infty} V^{k}_{\Delta t},
\end{align}
\end{subequations}

\noindent where $V_{\Delta t}(x)$ converges to $V(x)$ as $\Delta t \rightarrow 0$. Since the value function is being solved on a grid it can be represented in vectorized form, $\vec{V} \in \RR^{N_G}$, and the above equations can be written compactly as

\begin{subequations} \label{eq:dp_min_dist}
\begin{align}
&\vec{V}_{i}^{0} = l(x_i)\\
&\vec{V}_{i}^{k+1} = \min\{l(x_i),  \underset{u\in\U}{\max}\text{ }\underset{ d\in\D}{\min} I[\vec{V}^{k}](x+\Delta t f(x,u,d)) \label{eq:dp_min_dist_b}\}\\
&\vec{V} = \lim_{k\rightarrow \infty} \vec{V}^{k},
\end{align}
\end{subequations}

\noindent for  $i=1, ..., N_G$, where $\{x_i\}_{i=1}^{N_G}$ are the grid nodes, $\vec{V}_i^k$ is the approximate value for $V(x_i, k \Delta t)$ and $I[\vec{V}^k]:\RR^n \rightarrow \RR$ represents an interpolation operator defining, for every point $x$, the polynomial reconstruction based on the values $\vec{V}_i^k$. To recap one can obtain an approximation to the value function $V$ by initializing a grid $G$ with the values $l(x_i)$ at the grid nodes $\{x_i\}_{i=1}^{N_G}$, and then recursively updating the grid values using equation (\ref{eq:dp_min_dist_b}) until convergence. This is a standard dynamic programming solution, and equation \ref{eq:dp_min_dist_b} is referred to as the Bellman operator. Note that $k$ here is associated with the number of time steps, in particular the time horizon is $k \Delta t$.

\subsubsection{Bellman Operator}
For convenience we will formally define the Bellman operator $B[\cdot]: \RR^{N_G} \rightarrow \RR^{N_G}$, which maps the space of vectorized value functions onto itself. To do so compactly we introduce a few ideas. First, the \emph{control policy} $\pi_u(\cdot): \R^n \rightarrow \U$ and \emph{disturbance policy} $\pi_d(\cdot): \R^n \rightarrow \D$, which map from state to control and disturbance, respectively. The individual minimax games being played at each state can thus be collectively thought of as a game over policies. In addition, we note that since we use a linear interpolation scheme the interpolation function $I[\vec{A}](\cdot)$ is given by a policy-dependent convex combination over the elements of $\vec{A}$,

\begin{equation}
I[\vec{A}](x)= \vec{p}_{\pi_u,\pi_d}(x)^T \vec{A},
\end{equation}

\noindent where the elements of $\vec{p}_{\pi_u,\pi_d}(\cdot)^T$ are greater than zero and sum to 1. 

The Bellman operator is thus defined as 
\begin{equation} \label{eq:bell}
 B[\vec{A}] := \min\{\vec{l},  \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} P_{\pi_u, \pi_d} \vec{A}\},
\end{equation}

\noindent where $\vec{l} \in \RR^{N_G}$ with $\vec{l}_i = l(x_i)$ for $i=1, ..., N_G$, $P_{\pi_u, \pi_d} \in \RR^{N_G \times N_G}$ is a policy-dependent stochastic matrix.

With the Bellman operator in place, we can express equation (\ref{eq:dp_min_dist}) more compactly as

\begin{subequations}\label{eq:dp_bellman}
\begin{align}
&\vec{V}^{0} = \vec{l}\\
&\vec{V}_{i}^{k+1} = B[V^k] \\
&\vec{V} = \lim_{k\rightarrow \infty} \vec{V}^{k}.
\end{align}
\end{subequations}

Many optimal control problems are solved using the approach outlined by equation (\ref{eq:dp_bellman}), albeit with different initializations and Bellman operators. If the Bellman operator is a contraction mapping, then the initialization can be arbitrary. Later on we will leverage this in a new algorithm for computing safe sets online. 

\section{Bellman Operator}

%Perhaps include in introduction
%As it has been presented thus far the safe set is dependent on the underlying system model. In actuality the model is our best representation of things that are observed about the system. In that sense our model is an estimate of the system, and as more observations are collected this estimate is subject to change. The goal of learning is to leverage observations in order to produce models that better represent a given system. In this context an eventual consequence of learning the system model, is to learn the system's safe set. Why is this the case? For practical reasons the safe set is used to place restrictions on where the system is allowed to go (in the state space that is), and for the sake of maximizing system performance (in regards to other objectives) it is important that these restrictions are only as stringent as they need to be to ensure safe operation of the system. As one learns more about the system through observations the safe set should be updated to reflect this new knowledge.
%
%The simplest way to do so is to employ \ref{eq:dp_bellman} every time the model updates. However, this procedure can be quite costly, if one considers that performing dynamic programming just once is very expensive. Can we do better than this? To answer this question we will turn to the field of Reinforcement Learning (RL). One of the major tracks of traditional RL is concerned with learning a value function (albeit for a different optimal control problem). Many of the algorithms developed in the field work to iteratively modify the value function as new observations are made, as opposed to performing dynamic programming multiple times. This is all possible because the underlying Bellman operator that is induced is a contraction mapping, meaning that it does not matter how the value function $\vec{V_0}$ is initialized, after enough applications of the Bellman operator convergence to the correct value function is guaranteed. This means that as our model estimate changes we can always initialize the Bellman operator with the current best estimate of the value function. The benefits of doing this will be made clear in a future section, but for now we will focus on deriving a convergent Bellman Operator. We begin by analyzing the continuous time infinite horizon sum of discounted rewards functional, which is similar to the functional being optimized in RL.

When the Bellman operator is a contraction mapping, then the Dynamic Programming algorithm can be initialized arbitrarily. Furthermore, if the initialization is close to the solution then the algorithm will converge in fewer iterations. In an online setting where the safe set is being computed each time the model estimate changes, then one might expect that value functions of older model estimates are good initializations for computing the value function associated with the current model estimate. 

However, the Bellman operator in equation(\ref{eq:dp_bellman}) is not a contraction mapping. The form of the Bellman operator depends on the payoff function. In this section we will tweak the payoff function to induce a Bellman operator that is a contraction mapping. To gain insight, we first examine a payoff function that yields a contraction mapping: sum of discounted rewards.

\subsection{Sum of Discounted Rewards}

Take the following continuous time infinite horizon sum of discounted rewards functional

\begin{equation}\label{eq:V_sum_rewards}
\mathcal{V}\big(x,\bu(\cdot),\bdelta(\cdot)\big) := \int_{0}^{\infty}r\big(\bx_{x}^{\bu,\bdelta}(t)\big)\exp(-\lambda t) dt,  \quad \lambda >0,
\end{equation}

\noindent where $g(\cdot):\RR^n \rightarrow \RR$ is a state-dependent reward function, and $\lambda$ is a discount factor. Similar to before we can define a value function that encodes a game between the control signal and disturbance signal with equation (\ref{eq:V_sum_rewards}) as the payoff.

\begin{equation}
\V(x)=\inf_{\beta[\bu](\cdot) \in \B} \sup_{\bu \in \UU}\mathcal{V}\big(x,\bu(\cdot),\bdelta(\cdot)\big).
\end{equation}

It is known that the value function is the solution to the time-independent Hamilton-Jacobi partial differential equation,

\begin{equation}
\lambda V(x) = \max_{u\in\U} \min_{ d\in\D} \frac{\partial V}{\partial x}(x) f(x, u, d)+ g(x).
\end{equation}

Again assuming a semi-Lagrangian discretization over a grid, the PDE can be approximated as

\begin{equation}
V_{\Delta t}(x) = \max_{u\in\U} \min_{ d\in\D} \gamma V_{\Delta t}(x + \Delta t f(x, u, d))  + \Delta t g(x),
\end{equation}
 
\noindent where $\gamma=\exp(-\lambda \Delta t)$ is the discount rate. The approximation can then be solved for recursively


\begin{subequations}\label{eq:dp_bellman_sum}
\begin{align}
&\vec{V}^{0} \in \RR^N_G\\
&\vec{V}_{i}^{k+1} = B[V^k] := \vec{g} +  \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} \gamma P_{\pi_u, \pi_d} \vec{V}^k  \\
&\vec{V} = \lim_{k\rightarrow \infty} \vec{V}^{k}.
\end{align}
\end{subequations}

\noindent where $\vec{g}_i =\Delta t g(x_i)$ and $B[\cdot]$ is the Bellman operator associated with the sum of discounted reward functional. Note that $\vec{V}^{0}$ is initialized arbitrarily, which is due to the fact that the Bellman operator in this setting is a contraction mapping in the infinity norm, $||\cdot||_\infty$, thus it has a unique fixed-point. When the Bellman operator is a contraction mapping the procedure highlighted in equation (\ref{eq:dp_bellman_sum}) is also referred to as \emph{value iteration}.

\begin{definition} A mapping $M(\cdot): \RR^{N_G} \rightarrow \RR^{N_G}$, is said to be a contraction mapping in the norm $|| \cdot ||$ if there exists  a Lipschitz constant $0\geq \beta < 1$ such that for any $\vec{A}_1, \vec{A}_2 \in \RR^{N_G}$ $||M(\vec{A}_1) - M(\vec{A}_2)|| \leq \beta ||\vec{A}_1 - \vec{A}_2||$. 
\end{definition}

The contractive Bellman operator is a result of the discounting, in fact $\gamma$ is the Lipschitz constant. Discounting reduces the impact of future events on the payoff of the trajectory. The value function at any iteration can be thought of as an estimate of the impact of all future events, and the Bellman operator can be seen as combining these future events with the event taking place now ($\vec{g}$). Discounting ``forgets" things that happen far into the future, which includes any errors that we have in our initial estimate $\vec{V}^0$.  

\subsection {Minimum of Discounted Distances}
Inspired by the sum of discounted rewards payoff, we attempt to incorporate discounting into the minimum distance payoff in equation (\ref{eq:min_dist_functional}).

\subsubsection{The Right Payoff}

A first proposal for incorporating discounting might be the following payoff

\begin{equation}\label{eq:V}
\inf_{t\ge 0}l\big(\bx_{x}^{\bu,\bdelta}(t)\big)\exp(-\lambda  t).
\end{equation}

However, the discounting does not have the desired outcome in this case. Recall, that discounting should reduce the impact of future events on the payoff of the trajectory. The signed distance function is positive outside of the target, and negative inside, so once discounting is applied then states inside the target have a larger ``effective" signed distance further in the future, and states outside the target have a smaller effective signed distance. Since we are taking the minimum of the effective distance along the entire trajectory, then the event of being inside the target becomes less impactful to the payoff the later it happens in the trajectory (since the effective distance becomes more positive), but the event of being inside the target becomes more impactful the later it happens (since the effective distance becomes more negative).

One solution is to make the quantity being discounted negative everywhere, so that all events have a more positive effective distance in the future, and are thus less impactful when taking the minimum over the entire trajectory. Take an upper bound on the signed distance function $L$, such that $L>l(x), x \in \X $, where $\X \subset \RR^n$ is a subset of the state space that we wish to compute the value function on. The \emph{discounted minimum distance functional} is defined as 

\begin{equation}\label{eq:V_lambda}
\mathcal{V}_{\lambda}\big(x,\bu(\cdot),\bdelta(\cdot)\big) := L + \inf_{t\ge 0}(l\big(\bx_{x}^{\bu,\bdelta}(t)\big)-L)\exp(-\lambda  t).
\end{equation}

The term in the infemum is always nonpositive, so events have a smaller impact on the payoff the further in the future they occur. Note that if the discounting is removed ($\lambda =0$) then we get back to the original minimum distance functional. Using the above functional as the payoff for the game we can also define the following value function,

\begin{equation}
V_{\lambda}(x)=\inf_{\beta[\bu](\cdot) \in \B} \sup_{\bu \in \UU}\mathcal{V}_{\lambda}\big(x,\bu(\cdot),\bdelta(\cdot)\big).
\end{equation}


Since $L$ is an additive constant, our attention can be focused on the payoff from the second term of equation (\ref{eq:V_lambda}), which yields a related value function

\begin{equation}
U_{\lambda}(x)=\inf_{\beta[\bu](\cdot) \in \B} \sup_{\bu \in \UU}\mathcal{V}_{\lambda}\big(x,\bu(\cdot),\bdelta(\cdot)\big)-L, 
\end{equation}

\noindent thus $V_{\lambda}(x) = U_{\lambda}(x)+L$. It can be shown that the above value function is the viscosity solution to the following variational equality

\begin{equation}\label{eq:HJI_lambda}
    0 = \min\left\{l(x)-L-U_{\lambda}(x), \max_{u\in\U} \min_{ d\in\D} \!\!\frac{\partial U_{\lambda}}{\partial x}(x) f(x,u, d) - \lambda U_{\lambda}(x)\right\}.
\end{equation}

Following a semi-Lagrangian approach, the discrete approximation of the PDE is given by

\begin{equation}\label{eq:U_lambda_approx}
    U_{\Delta t} (x) = \min\left\{l(x)-L, \max_{u\in\U} \min_{ d\in\D}  \gamma U_{\Delta t}(x+\Delta tf(x,u,d))\right\},
\end{equation}

\noindent which can be solved recursively


\begin{subequations}\label{eq:dp_bellman_lambda}
\begin{align}
&\vec{U}^{0} \in \RR^N_G\\
&\vec{U}_{i}^{k+1} = B_\lambda[U^k] := \min\left\{ \vec{h}, \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} \gamma P_{\pi_u, \pi_d} \vec{U}^k \right \}  \\
&\vec{U} = \lim_{k\rightarrow \infty} \vec{U}^{k}
\end{align}
\end{subequations}

\noindent where $\vec{h}_i = l(x_i)-L$ and $B_{\lambda}[\cdot]$ is the Bellman operator for this payoff. Note that $\vec{U}^{0}$ is initialized arbitrarily, which is due to the fact that the Bellman operator $B_{\lambda}[\cdot]$ is a contraction mapping with a unique fixed-point. 



\begin{lemma}\label{lem:maxmin}
\begin{equation} 
|\max_a \min_b f(a,b) -\max_a \min_b g(a,b)| \leq \max_a \min_b |f(a,b) - g(a,b)|
\end{equation}
\end{lemma}

\begin{proof}
Define the minimax optimizers for $f$ as the pair $(a_f,b_f)$, and minimax optimizers of $g$ as the pair $(a_g, b_g)$. Without loss of generality we assume that $f(a_f,b_f) > g(a_g,b_g)$.

\begin{equation*}
|\max_a \min_b f(a,b) -\max_a \min_b g(a,b)| \leq |f(a_f,b_f) - \min_b g(a_f,b)|
\end{equation*}

\noindent Next define $b_{gg} :=\arg\min_b g(a_f,b)$,

\begin{equation*}
\leq |f(a_f,b_{gg}) - g(a_f,b_{gg})| \leq \max_a \min_b |f(a,b) - g(a,b)|
\end{equation*}
\end{proof}


\begin{proposition} 
The Bellman operator $B_{\lambda}[\cdot]$ is a contraction mapping in the infinity norm $|| \cdot ||_{\infty}$, that is for any two vectors $\vec{A}_1, \vec{A}_2 \in \RR^{N_G}$,  $||B_{\lambda}[\vec{A}_1] - B_{\lambda}[\vec{A}_2]||_{\infty} < \beta||\vec{A}_1 - \vec{A}_2||_{\infty}$, $0\leq \beta <1$.
\end{proposition}
\begin{proof}
\begin{equation*}
||B_{\lambda}[\vec{A}_1] - B_{\lambda}[\vec{A}_2]||_{\infty} = ||\min\left\{ \vec{h}, \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} \gamma P_{\pi_u, \pi_d} \vec{A}_1 \right \}  - \min\left\{ \vec{h}, \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} \gamma P_{\pi_u, \pi_d} \vec{A}_2 \right \}||_{\infty}
\end{equation*}

\noindent Leveraging the identity $\min\{a,b\} = \frac{1}{2}((a+b)- |a-b|)$ and using the shorthand $\Pi[\vec{A}]=\underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} \gamma P_{\pi_u, \pi_d} \vec{A}$ ,

\begin{equation*}
= \frac{1}{2} ||(\Pi[\vec{A}_1]  - \Pi[\vec{A}_2] ) -  (|\Pi[\vec{A}_1]-\vec{h}|  - |\Pi[\vec{A}_2]-\vec{h}|)||_{\infty}.
\end{equation*}

\noindent From the triangle inequality,

\begin{equation*}
\leq \frac{1}{2} ||(\Pi[\vec{A}_1]  - \Pi[\vec{A}_2] )||_{\infty} + \frac{1}{2}  ||(|\Pi[\vec{A}_1]-\vec{h}|  - |\Pi[\vec{A}_2]-\vec{h}|)||_{\infty}.
\end{equation*}

\noindent Given the inequality $|a-b| > |(|a|-|b|)|$, the second term in the inequality is upper bounded by the first term

\begin{equation*}
\leq ||(\Pi[\vec{A}_1]  - \Pi[\vec{A}_2] )||_{\infty}= ||\underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} \gamma P_{\pi_u, \pi_d}\vec{A}_1 - \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} \gamma P_{\pi_u, \pi_d} \vec{A}_2||_{\infty}.
\end{equation*}

\noindent Finally from Lemma \ref{lem:maxmin},
\begin{equation*}
\leq \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} ||\gamma P_{\pi_u, \pi_d} (\vec{A}_1 - \vec{A}_2)||_{\infty} \leq \gamma||\vec{A}_1 - \vec{A}_2||_{\infty},
\end{equation*}

\noindent where the last inequality comes from the fact that $P_{\pi_u, \pi_d}$ is a stochastic matrix for all policies, thus $||P_{\pi_u, \pi_d}||_{\infty} = 1$.
\end{proof}



\subsubsection{Characterizing the Safe Set}

Recall from equation (\ref{eq:safe_set}) that the safe set $\Omega(\K)$ was defined as the super-zero level set of the value function associated with the minimum distance payoff function, which we denote as  $V_0(\cdot)$ (i.e. $\lambda = 0)$. We need to characterize the safe set using the discounted value function $V_{\lambda}(\cdot)$.Note that $V_{\lambda}(x) \geq V_0(x) \quad \forall x\in \X$ because the terms being discounted are nonpositive. The difference between the value functions is likely state-dependent, so we need to find an appropriate level curve of the discounted value function that yields an under-approximation of the safe set. Stated differently, we need to find $\epsilon \geq 0$, such that if $V_0(x)<0$, then $V_\lambda(x)<\epsilon$. 

We can bound the difference between the two value functions.  Define $\tau(x)$ as the time when the minimum distance to the target is achieved for a trajectory starting at state $x$ under the optimal control and disturbance signals. Then we have the following bound

\begin{equation}
V_{\lambda}(x) - V_0(x)  \leq (L - l(\bx_{x}^{\bu,\bdelta}(\tau(x))))( 1 -  \exp(-\lambda \tau(x))) 
\end{equation}

Noting that $V_0(x)=l(\bx_{x}^{\bu,\bdelta}(\tau(x)))$, we get the resulting inequality

\begin{equation}
V_{\lambda}(x) -  L( 1 -  \exp(-\lambda \tau(x))) \leq V_0(x) \exp(-\lambda \tau(x)), 
\end{equation}

\noindent and the following implication

\begin{equation}
V_0(x)<0 \implies V_{\lambda}(x) <  L( 1 -  \exp(-\lambda \tau(x)))
\end{equation}

Assuming an upper bound  $\bar{\tau} \geq \tau(x)$, then $\epsilon = L( 1 -  \exp(-\lambda \bar{\tau}))$ is a valid choice.\footnote{It turns out that $\bar{\tau}$ does not need to bound the time that it takes to reach the minimum distance for any start state $x$, but rather the time that it takes to reach the target set for all states outside the target that ultimately reach the target.} Thus we can define the \emph{discounted safe set} as 

\begin{equation} \label{eq:safe_set_lambda}
\Omega_{\lambda}(\K) := \{x \mid V_{\lambda}(x) \ge L( 1 -  \exp(-\lambda \bar{\tau})) \} \subseteq \Omega(\K).
\end{equation}  

\section{Problem Formulation}
As mentioned earlier value functions are typically computed offline due to the computational overhead. However, in some cases we may want to compute new value functions online due to changes in the model being used to approximate the system. One approach is to generate a library of value functions for different models, which can then be used as a look up table. Generating this library is still costly, and there are no guarantees that the library will include the value function for a particular model of interest.

In general, for each new model encountered  a value function must be computed via dynamic programming. However, in the case where the system model belongs to a parameterized class, our objective is to develop a method for computing the value functions online, that is more efficient than employing dynamic programming with a fixed initialization. The metric we are concerned with when we speak of efficiency, is the amount of time it takes to produce a new safe set/value function after the model has converged. We assume that the model parameters ultimately converge because it would not be practical to utilize a safe set based on a model that we have little confidence in.

\subsection{Parameterized Models}
Let's consider a dynamics model from a  parameterized class, $\dot{x}=f_{\theta}(x,u,d)$ for $\theta \in \Theta$. The parameters can represent a number of things like physical attributes (mass, moment of inertia, etc.), the intentionality of an underlying agent (parameterized policy), or even stochasticity (parameterized process noise). This also implies that the Bellman operator and value function are parameterized, so we have $B_\theta$ and $V_\theta$ respectively. Note that for now we are talking about the Bellman operator and value function generally and not tied to a particular payoff. Though the parameter value is changing we only require the safe set for the converged parameter.

\section{Methodology}
Computing the safe set online, after the model has converged, could cause a significant delay between when the model parameter converges, and when the associated safe set is produced. 

We propose a method that draws inspiration from reinforcement learning (RL). In RL local updates are made to the value function based on the current observations being made. As new observations are made, more updates are made. In our method the safe set gets updated globally at every time step using the Bellman operator of the current parameter estimate. Rather than the observation influencing the value function directly, it influences the model estimate, which in turn influences the value function through one Bellman update. One Bellman update is far cheaper than computing the safe set, which requires applying the operator until convergence. In addition as the parameter estimate converges, this procedure ultimately converges to the appropriate safe set.   

\subsection{Dynamic Bellman Operator}

%Before going into the algorithm let's take some time to understand the rationale behind it. If we assume that the mapping from parameter $\theta$ to value function $\vec{V}_{\theta}$ is continuous, then informally we expect if two parameter values $\theta_1$ and $\theta_2$ are close together then their safe sets will also be close. It can be shown that the value iteration algorithm in the previous section has linear convergence.
%
%\begin{equation}
%||\vec{U}-\vec{U}^k|| \leq \gamma^k ||\vec{U}-\vec{U}^0||,
%\end{equation}
%
%\noindent where $\gamma= e^{-\lambda \Delta t}$, and the metric is with respect to the infinity norm. Clearly, the closer $\vec{U}^0$ is to $\vec{U}$ the lower the maximum number of iterations it takes to converge. Thus in performing value iteration with $B_{\theta_2}$ a good initialization would be $\vec{U}^0=\vec{U}_{\theta_1}$. Furthermore, any value function close to $\vec{U}_{\theta_1}$ would also be a good initialization, thus even if we applied value iteration under $B_{\theta_1}$ and terminated before convergence the result would be useful in finding $\vec{U}_{\theta_2}$.

Given an estimation process that returns a convergent sequence of parameter estimates $\{\theta^k\}$  with $\lim_{k \rightarrow \infty} \theta_k = \theta^*$ we propose a dynamic online Bellman update scheme 

\begin{equation} \label{eq:dyn_bell}
\vec{V}^{k+1}=B_{\theta_k}[\vec{V}^k],
\end{equation}

\noindent where $\vec{V}^{0}$ is set arbitrarily and $B_\theta[\cdot]$ is a contraction mapping in the infinity norm with Lipschitz constant $\beta$ for all $\theta$. The hope behind this update rule is that  once the parameter $\theta$ converges it should not require many more Bellman updates to produce the desired value function. 

\begin{proposition} 
Assuming that the vectorized value function $\vec{V}_\theta$ is continuous in $\theta$ and that $\exists C<\infty$ such that $C>||\vec{V}^k||_{\infty}$ for all $k$ and $C>||\vec{V}_{\theta^*}||$, then the sequence $\{V^k\}$ converges to $\vec{V}_{\theta^*}$.
\end{proposition}

\begin{proof}
To prove convergence it must be shown $\forall \epsilon>0$ $\exists K$ such that for all $k>K$ $||\vec{V}_{\theta^*}-\vec{V}^{k}||_{\infty}< \epsilon$. Note that the sequence $\{\theta^k\}$ produces a sequence of value functions $\{\vec{V}_{\theta^k}\}$, which converges to $\vec{V}_{\theta^*}$ due to the continuity of the vectorized value function in $\theta$.

\begin{equation*}
||\vec{V}_{\theta^*}-\vec{V}^{k}||_{\infty} \leq ||\vec{V}_{\theta^*}-\vec{V}_{\theta^{k-1}}||_{\infty} + ||\vec{V}_{\theta^{k-1}}-\vec{V}^{k}||_{\infty}
\end{equation*}

\noindent Since $\{\vec{V}_{\theta^k}\}$ converges to $\vec{V}_{\theta^*}$ we can find $K_1$ such that for $m>K_1$ $||\vec{V}_{\theta^*}-\vec{V}_{\theta^{m}}||_{\infty} < \epsilon'$, so

\begin{equation*}
\leq \epsilon' + ||\vec{V}_{\theta^{k-1}}-\vec{V}^{k}||_{\infty} = \leq \epsilon' + ||B_{\theta^{k-1}}[\vec{V}_{\theta^{k-1}}]-B_{\theta^{k-1}}[\vec{V}^{k-1}]||_{\infty}.
\end{equation*}

\begin{equation*}
\leq \epsilon' + \beta ||\vec{V}_{\theta^{k-1}}-\vec{V}^{k-1}||_{\infty},
\end{equation*}

\noindent which comes from $B_{\theta}$ being a contraction mapping. Using triangle inequality,

\begin{equation*}
\leq \epsilon' + \beta ||\vec{V}_{\theta^{k-1}}-\vec{V}_{\theta^{*}}||_{\infty} + \beta ||\vec{V}_{\theta^{*}}-\vec{V}^{k-1}||_{\infty} \leq (1+\beta)\epsilon' + \beta ||\vec{V}_{\theta^{*}}-\vec{V}^{k-1}||_{\infty},
\end{equation*}

\noindent Bringing it all together we have the recursively defined inequality

\begin{equation*}
||\vec{V}_{\theta^*}-\vec{V}^{k}||_{\infty}\leq (1+\beta)\epsilon' + \beta ||\vec{V}_{\theta^{*}}-\vec{V}^{k-1}||_{\infty}.
\end{equation*}

\noindent Recursing back to iteration $K_1$ we get 

\begin{equation*}
||\vec{V}_{\theta^*}-\vec{V}^{k}||_{\infty}\leq \epsilon'(1+\beta) \sum_{j=0}^{k-K_1-1} \beta^j + \beta^{k-K_1} ||\vec{V}_{\theta^{*}}-\vec{V}^{K_1}||_{\infty}\leq \frac{\epsilon' }{(1-\beta^2)} + 2\beta^{k-K_1} C .
\end{equation*}

\noindent  Choose $K_1$ such that  $\epsilon' =\frac{\epsilon(1-\beta^2)}{2}$ and choose $K= \frac{\log(\frac{\epsilon}{4C})}{\log(\beta)} +K_1$ and then the desired result is acheived

\begin{equation*}
||\vec{V}_{\theta^*}-\vec{V}^{k}||_{\infty}\leq \epsilon.
\end{equation*}
\end{proof}


\subsection{Discounted Minimum Distance Bellman Operator}
In the previous subsection we proved the convergence of a dynamic Bellman update scheme under certain assumptions. In this section we will prove that the Bellman operator given by equation (\ref{eq:dp_bellman_lambda}) satisfies the assumptions. In this section the Bellman operator $B_{\theta}[\cdot]$ is defined as

\begin{equation}
B_{\theta}[\vec{V}] := \min\left\{ \vec{h}, \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} \gamma P^{\theta}_{\pi_u, \pi_d} \vec{V} \right \},
\end{equation}

\noindent where $P^{\theta}_{\pi_u, \pi_d}$ is a stochastic matrix that is continuous in $\theta$ and every Bellman operator has a fixed point $B_{\theta}[\vec{V}_{\theta}]=\vec{V}_{\theta}$.\footnote{The matrix $P^{\theta}_{\pi_u, \pi_d}$ will be continuous in $\theta$ for all $\pi_u(\cdot)$ and $\pi_d(\cdot)$ as long as $f_{\theta}(x,u,d)$ is continuous in $\theta$. The elements in $P^{\theta}_{\pi_u, \pi_d}$ are simply the weights of our interpolation scheme over the space, so if we move over the space continuously, then the weights also change continuously.}

First, we state a few simple lemmas.
\begin{lemma}
The function $g_1(\theta,\pi_u,\pi_d,\vec{V})=  P^{\theta}_{\pi_u, \pi_d} \vec{V}$ is continuous in $\theta$ for all $\vec{V}\in \RR^{N_G}$.
\end{lemma}

\begin{proof}
This comes from the continuity of $P^{\theta}_{\pi_u, \pi_d}$ in $\theta$. The elements of the output from function $g_1(\theta,\pi_u,\pi_d,\vec{V})$ are just linear combinations of continuous elements of $P^{\theta}_{\pi_u, \pi_d}$ (continuous in $\theta$ that is).
\end{proof}

\begin{lemma}
The function $g_2(\theta,\vec{V})= \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} P^{\theta}_{\pi_u, \pi_d} \vec{V}$ is continuous in $\theta$ for all $\vec{V}\in \RR^{N_G}$.
\end{lemma}

\begin{proof}
Note $g_2(\theta,\vec{V})=\underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min}g_1(\theta,\pi_u,\pi_d,\vec{V})$. The minimax of continuous functions is continuous. 
\end{proof}


\begin{lemma}
The function $g_3(\theta,\vec{V})= \min\{\vec{h}, \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} P^{\theta}_{\pi_u, \pi_d} \vec{V}\}$ is continuous in $\theta$ for all $\vec{V}\in \RR^{N_G}$.
\end{lemma}

\begin{proof}
Note $g_3(\theta,\vec{V})= \min\{\vec{h}, g_2(\theta,\vec{V})\}$. The minimum of continuous functions is also continuous. 
\end{proof}

\begin{proposition}
The fixed point of the Bellman operator $B_{\theta}[\cdot]$, which we denote as $\vec{V}_{\theta}$ such that $\vec{V}_{\theta}=B_{\theta}[\vec{V}_{\theta}]$, is continuous in $\theta$.
\end{proposition}

\begin{proof}
We need to show that for any $\theta$ $\forall \epsilon>0$ $\exists \delta>0$ such that if $||\theta'-\theta|| < \delta$ then $||\vec{V}_{\theta'} - \vec{V}_{\theta}||_{\infty} < \epsilon$. Take the sequence generated by $\vec{V}^0=\vec{V}_{\theta}$,  $\vec{V}^{k+1}=B_{\theta'}[\vec{V}^{k}]$, which converges to $\vec{V}_{\theta'}$. By triangle inequality we have

\begin{equation*}
||\vec{V}_{\theta'} - \vec{V}_{\theta}||_{\infty} \leq \sum_{k=0}^{\infty}||\vec{V}^{k+1}-\vec{V}^{k}||_{\infty}.
\end{equation*}

\noindent We can recursively apply the fact that $B_{\theta'}$ is a contraction mapping, 

\begin{equation*}
\leq \sum_{k=0}^{\infty}\gamma^k||\vec{V}^{1}-\vec{V}^{0}||_{\infty} = \frac{1}{1-\gamma}||\vec{V}^{1}-\vec{V}^{0}||_{\infty}
\end{equation*} 

\begin{equation*}
=\frac{1}{1-\gamma}||B_{\theta'}[\vec{V}_{\theta}]-\vec{V}_{\theta}||_{\infty}=\frac{1}{1-\gamma}||B_{\theta'}[\vec{V}_{\theta}]-B_{\theta}[\vec{V}_{\theta}]||_{\infty}
\end{equation*}

\noindent Note that $B_{\theta}[\vec{V}]=g_3(\theta,\vec{V})$, so it is continuous in $\theta$, and thus for a given $\epsilon'>0$ we can choose $\delta>0$ such that 

\begin{equation}
\leq \frac{\epsilon'}{1-\gamma},
\end{equation}

\noindent so choose $\epsilon' = \epsilon (1-\gamma)$.

\end{proof}

\end{document}
