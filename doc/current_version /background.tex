% !TEX root = main_min_disc_dist.tex
In this section we consider a two player differential game where the first player (control) wants to keep the system away from the target set for a given time horizon and the second player (disturbance) has the opposite objective.\footnote{We can also consider the case where the objectives are switched by making minor modifications to the material presented here.} In addition, a methodology is presented for characterizing the infinite horizon backwards reachable set, which is the set of initial states for which the disturbance wins the game. Going forward all mentions of reachable set refer to the infinite horizon backwards reachable set.

\subsection{System Model \label{subsec:dynamics}}

The analysis in this paper considers a fully observable system whose underlying dynamics may be non-deterministic, but bounded. 
We can formalize this as a dynamical system with state $x\in\RR^n$, and two inputs, $u\in\U\subset\RR^{n_u},  d\in\D\subset\RR^{n_d}$
(with $\U$ and $\D$ compact)
which we will refer to as the \emph{controller} and the \emph{disturbance}:
\begin{equation}\label{fxud}
\dot{x} = f(x,u, d).
\end{equation}
In this context, however, 
$d$ is thought of as a disturbance capturing the non-deterministic or unknown portions of the dynamics, and the flow field $f: \RR^n \times \U \times \D\rightarrow\RR^n$ is assumed to be uniformly continuous and bounded. Later we will consider working with a parameterized class of flow fields.

Letting $\UU $ and $\DD$ denote the collections of measurable%
	\footnote{A function $f:X\to Y$ between two measurable spaces $(X,\Sigma_X)$ and $(Y,\Sigma_Y)$
	is said to be measurable if the preimage of a measurable set in $Y$ is a measurable set in $X$, that is:
	$\forall V\in\Sigma_Y, f^{-1}(V)\in\Sigma_X$, with $\Sigma_X,\Sigma_Y$ $\sigma$-algebras on $X$,$Y$.}
functions $\bm u: [0,\infty)\to \U $ and $\bm d: [0,\infty)\to \D$ respectively,
and allowing the controller and disturbance to choose any such signals,
the evolution of the system
from any initial state $x$
is determined (see for example \cite{Coddington1955}, Ch. 2, Theorems 1.1, 2.1) by the unique continuous trajectory $\bx:[0,\infty)\to\RR^n$ solving
\begin{equation}\label{eq:xdot}
\begin{split}
\dot{\bx}(s) &= f(\bx(s),\bu(s),\bdelta(s)), \text{ a.e. }s\ge 0,\\
\bx(0) &= x.
\end{split}
\end{equation}
Note that this is a solution in Carath\'eodory's \emph{extended sense}, that is, it satisfies the differential equation \emph{almost everywhere} (i.e. except on a subset of Lebesgue measure zero).


Throughout our analysis, we will use the notation $\bx_{x}^{\bu,\bdelta}(\cdot)$ to denote the state trajectory $t\mapsto x$ corresponding to the initial condition $x\in\RR^n$, the control signal $\bu\in\UU$ and the disturbance signal $\bdelta\in\DD$.


\subsection{HJI Reachability: Minimum Distance to Target}
The target set $\T$ can be implicitly characterized as the sub-zero level set of a Lipschitz \emph{surface function} $l:\RR^n\rightarrow\RR$: 
\begin{equation}\label{eq:l}
x\in\T\iff l(x)<0.
\end{equation}
This function always exists, since we can simply choose the \emph{signed distance function} to $\T$,  $s_{\T}(x)$, which is Lipschitz continuous by construction.\footnote{ For any nonempty set $\M\subset\RR^m$, the signed distance function $s_{\M}:\RR^m\to\RR$  is defined as
$\inf_{y\in\M} |z-y|$ for points outside of $\M$ and $ -\inf_{y\in\RR^m\setminus\M} |z-y|$ for points inside $\M$, where $|\cdot|$ denotes a norm on $\RR^m$.} 
Alternatively, we use a clipped signed distance since the problem is solved over a fixed domain, $l(x) = \min(\max(s_{\T}(x), -L), L)$ with $L>0$.

To express whether a given trajectory \emph{ever} enters the target set, let the functional $\V:\RR^n\times\UU\times\DD\to\RR$ assign to each initial state $x$ and input signals $\bu(\cdot)$, $\bdelta(\cdot)$ the lowest value of $l(\cdot)$ achieved by trajectory $\bx_{x}^{\bu}(\cdot)$ over all times $t\ge0$: 
\begin{equation}\label{eq:min_dist_functional}
\mathcal{V}\big(x,\bu(\cdot),\bdelta(\cdot)\big) := \inf_{t\ge 0}l\big(\bx_{x}^{\bu,\bdelta}(t)\big).
\end{equation}
This outcome $\V$, also referred to here as the \emph{minimum of rewards}\footnote{In this context reward refers to $l(x)$, but in general it can be any scalar valued function}, will be nonpositive if there exists any $t\in[0,\infty)$ at which the trajectory enters the target set, and will be strictly positive if the system remains in the constraint set for all of $t\ge 0$. Denoting $\V^{\bu,\bdelta}(x) = \V\big(x,\bu(\cdot),\bdelta(\cdot)\big)$, the following statement follows from \eqref{eq:l} and \eqref{eq:min_dist_functional} by construction. 

\begin{proposition}\label{Value}
The set of points $x$ from which the system trajectory $\bx^{\bu,\bdelta}_{x}(\cdot)$ under given inputs $\bu(\cdot)\in\UU,\bdelta(\cdot)\in\DD$ will enter the target set $\T$ for some time $t\ge0$ is equal to the zero sublevel set of $\mathcal{V}^{\bu,\bdelta}(\cdot)$:
\[
\{x\in\RR^n: \exists t\ge0,\; \bx^{\bu,\bdelta}_{x}(t)\in\T\}=\{x\in\RR^n: \mathcal{V}^{\bu,\bdelta}(\cdot)\le0\}.
\]
\end{proposition}


The reachable set can then be obtained by solving a differential game (or optimal control problem in the case where there is no disturbance) between the control and disturbance signal with the restriction that the disturbance signal can only use  \emph{nonanticipative strategies}. The set of nonanticipative strategies for the disturbance is $\B = \{\bbeta:\UU\to\DD\;|\;
\forall t\ge 0,\; \forall \bu(\cdot),\hat{\bu}(\cdot)\in\UU,$
${\big(\bu(\tau) \!=\! \hat{\bu}(\tau)\text{ a.e.} \tau\ge0\big)\Rightarrow
{\big(\bbeta[\bu](\tau) \!=\! \bbeta[\hat{\bu}](\tau)}{\text{ a.e.} \tau\ge0\big)}}\}$. With this in place we can define the \emph{value function} $V(x)$ and ultimately the reachable set $\R(\T)$. 

\begin{equation} \label{eq:val_function}
V(x)=\inf_{\beta[\bu](\cdot) \in \B} \sup_{\bu \in \UU}\mathcal{V}\big(x,\bu(\cdot),\bdelta(\cdot)\big),
\end{equation}

\begin{equation} \label{eq:reach_set}
\R(\T) = \{x \mid V(x) \le 0\}.
\end{equation}  

The optimaization in \eqref{eq:val_function} is referred to here as the minimum reward differential game optimal control problem; technically speaking it is actually a differential game. It has been shown that the value function for games with payoff given by equation \eqref{eq:min_dist_functional} can be characterized as the unique viscosity solution to a variational inequality \cite{Barron1989, Barron1990}. An alternative formulation involves a modified PDE\cite{Mitchell2005}. In a finite-horizon setting when the game is played over a finite interval the $[0,T]$, the \emph{finite-horizon value function} $V(x,t)$ can be computed by solving the Hamilton-Jacobi-Isaacs (HJI) equation: 

\begin{equation} \label{eq:HJI}
\begin{split} 
    & 0 = \min\left\{l(x)-V(x,t), \frac{\partial V}{\partial t}(x,t)+ \max_{u\in\U} \min_{ d\in\D} \!\!\frac{\partial V}{\partial x}(x,t) f(x,u, d)\right\}\\
    &V(x,T) = l(x).
\end{split}
\end{equation}


 As $T \rightarrow \infty$, $V(x,t)$ becomes independent of $t$. We accordingly drop the dependence on $t$ and recover $V(x)$ as defined in \eqref{eq:val_function}. 


\subsection{Computing Value Function}
Several approximation schemes have been proposed for solving (\ref{eq:HJI}) and similar PDEs and variational equalities on a fixed grid $G$ \cite{Bardi1999, Falcone1994, Mitchell2005, Osher2003, Sethian1996}. Here we will use a semi-Lagrangian approximation based on a Discrete Time Dynamic Programming Principle, which yields

\begin{subequations}
\begin{align}
&V_{\Delta t}^{k+1}(x) = \min\{l(x),  \underset{u\in\U}{\max}\text{ }\underset{ d\in\D}{\min} V^{k}_{\Delta t}(x+\Delta t f(x,u,d))\}\\
&V_{\Delta t}^{0}(x) = l(x)\\
&V_{\Delta t} = \lim_{k\rightarrow \infty} V^{k}_{\Delta t},
\end{align}
\end{subequations}

\noindent where $V_{\Delta t}(x)$ converges to $V(x)$ as the discrete time step $\Delta t \rightarrow 0$. Since the value function is being solved on a grid it can be represented in vectorized form, $\vec{V} \in \RR^{N_G}$, and the above equations can be written compactly as

\begin{subequations} \label{eq:dp_min_dist}
\begin{align}
&\vec{V}_{i}^{0} = l(x_i)\\
&\vec{V}_{i}^{k+1} = \min\{l(x_i),  \underset{u\in\U}{\max}\text{ }\underset{ d\in\D}{\min} I[\vec{V}^{k}](x_i+\Delta t f(x_i,u,d)) \label{eq:dp_min_dist_b}\}\\
&\vec{V} = \lim_{k\rightarrow \infty} \vec{V}^{k},
\end{align}
\end{subequations}

\noindent for  $i=1, ..., N_G$, where $\{x_i\}_{i=1}^{N_G}$ are the grid nodes, $\vec{V}_i^k$ is the approximate value for $V(x_i, k \Delta t)$ and $I[\vec{A}]:\RR^n \rightarrow \RR$ represents an interpolation operator defining, for every point $x$, the polynomial reconstruction based on the values $\vec{A}$. Unless stated otherwise $G$ is taken to be a regular equidistant array of points with mesh spacing $\Delta x_j$ along the $j^{th}$ axis, $j=1,...,n$.\footnote{In the most general case the control and disturbance sets are also discretized, $\U=\{u_i\}_{i=1}^{N_U}$ and $\D=\{d_i\}_{i=1}^{N_D}$, and the minimax game is approximated.} In addition, a multilinear interpolator is used for the interpolation scheme, thus the interpolation function $I[\vec{A}](\cdot)$ is given by a convex combination over the elements of $\vec{A}$,
\begin{equation}
I[\vec{A}](x)= \vec{p}(x)^T \vec{A},
\end{equation}

\noindent where the elements of $\vec{p}(\cdot)$ are greater than zero and sum to one. 

For conciseness we introduce \emph{control policies} $\pi_u(\cdot): \R^n \rightarrow \U$ and \emph{disturbance policies} $\pi_d(\cdot): \R^n \rightarrow \D$, which map from state to control and state to disturbance, respectively. Note that the individual minimax games being played at each grid point can be collectively thought of as a game over policies. We also define the operator $B[\cdot]: \RR^{N_G} \rightarrow \RR^{N_G}$

\begin{equation} \label{eq: op_min_dist}
 B[\vec{A}] := \min\{\vec{l},  \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} P_{\pi_u, \pi_d} \vec{A}\},
\end{equation}

\noindent where $\vec{l} \in \RR^{N_G}$ with $\vec{l}_i = l(x_i)$ for $i=1, ..., N_G$, $P_{\pi_u, \pi_d} \in \RR^{N_G \times N_G}$ is a policy-dependent stochastic matrix\footnote{$P_{\pi_u, \pi_d}$ is effectively the probability transition matrix for a Markov Decision Process (MDP) over a finite state space given by the grid nodes $\{x_i\}_{i=1}^{N_G}$.}, and row $i$ of $P_{\pi_u, \pi_d}$ is $\vec{p}(x_i+\Delta tf(x, \pi_u(x_i), \pi_d(x_i)))$. We can now express \eqref{eq:dp_min_dist} more concisely as

\begin{subequations}\label{eq:dp_one}
\begin{align}
&\vec{V}^{0} = \vec{l}\\
&\vec{V}_{i}^{k+1} = B[V^k] \\
&\vec{V} = \lim_{k\rightarrow \infty} \vec{V}^{k}.
\end{align}
\end{subequations}

Equation \eqref{eq:dp_one} represents the DP algorithm associated with the differential game. We refer to $\vec{V}$ as the \emph{vectorized value function}, and approximate the value function $V(x)$ with $I[\vec{V}](x)$. 

\subsection{Contraction Mappings and Sum of Discounted Rewards}

As stated in the introduction if the DP algorithm is a contraction mapping, then then we have a great deal of flexibility in how the algorithm is initialized.  

\begin{definition} A mapping $M(\cdot): \RR^{N_G} \rightarrow \RR^{N_G}$, is said to be a contraction mapping in the norm $|| \cdot ||$ over the space $\RR^{N_G}$ if there exists a Lipschitz constant $0\leq \beta < 1$ such that for any $\vec{A}_1, \vec{A}_2 \in \RR^{N_G}$, $||M(\vec{A}_1) - M(\vec{A}_2)|| \leq \beta ||\vec{A}_1 - \vec{A}_2||$. 
\end{definition}

By the contraction mapping theorem any contraction mapping has a unique fixed point. The operator given by \eqref{eq: op_min_dist} is not a contraction mapping. To see this note that any vector $\alpha \vec{1} \in \RR^{N_G}$ is a fixed point for $\alpha < -L$, where $\vec{1}= [1, 1, ..., 1]^T$ is the vector of ones and $-L$ is the lower bound on the clipped signed distance function.

The SDR payoff does admit a contraction mapping, and we explore it briefly:

\begin{equation}\label{eq:V_sum_rewards}
\mathcal{V}\big(x,\bu(\cdot),\bdelta(\cdot)\big) := \int_{0}^{\infty}r\big(\bx_{x}^{\bu,\bdelta}(t)\big)\exp(-\lambda t) dt,  \quad \lambda >0,
\end{equation}

\noindent where $r(\cdot):\RR^n \rightarrow \RR$ is a state-dependent reward function, and $\lambda$ is a \emph{discount factor}. 

We can define a value function $V(x)$ using equation \eqref{eq:val_function}, and in this instance $V(x)$ is the unique viscosity solution to a time-independent HJ PDE 

\begin{equation}
\V(x)=\inf_{\beta[\bu](\cdot) \in \B} \sup_{\bu \in \UU}\mathcal{V}\big(x,\bu(\cdot),\bdelta(\cdot)\big).
\end{equation}

It is known that the value function is the solution to the time-independent Hamilton-Jacobi partial differential equation \cite{Bardi2008},

\begin{equation}
\lambda V(x) = \max_{u\in\U} \min_{ d\in\D} \frac{\partial V}{\partial x}(x) f(x, u, d)+ g(x).
\end{equation}

Using a semi-Lagrangian scheme, the PDE can be approximated as

\begin{equation}
V_{\Delta t}(x) = \max_{u\in\U} \min_{ d\in\D} \gamma V_{\Delta t}(x + \Delta t f(x, u, d))  + \Delta t r(x),
\end{equation}
 
\noindent where $\gamma=\exp(-\lambda \Delta t)$ is the \emph{discount rate}.

The approximation is then given by the following sequence

\begin{subequations}\label{eq:dp_sum}
\begin{align}
&\vec{V}_{i}^{k+1} = B[V^k] := \vec{r} +  \underset{\pi_u}{\max}\text{ }\underset{ \pi_d}{\min} \gamma P_{\pi_u, \pi_d} \vec{V}^k  \label{eq:dp_sum_a}\\
&\vec{V} = \lim_{k\rightarrow \infty} \vec{V}^{k}.
\end{align}
\end{subequations}

\noindent where $\vec{r}_i =\Delta t r(x_i)$. Here $\vec{V}^{0} \in \RR^N_G$ is initialized arbitrarily since the DP operator \eqref{eq:dp_sum_a} is a contraction mapping in the infinity norm, $||\cdot||_\infty$ \cite{Bertsekas1995}. Equation \eqref{eq:dp_sum} is also referred to as \emph{value iteration}.

Discounting reduces the impact of future rewards on the payoff of the trajectory, and ultimately yields a contraction mapping.\footnote{The discount rate $\gamma$ is the Lipschitz constant of the contraction mapping} The value function at any iteration can be thought of as an estimate of the impact of all future rewards, and the DP operator can be seen as combining these future rewards with the current reward $\vec{r}$. Essentially things that happen far into the future are ``forgotten", which includes errors in the initial estimate $\vec{V}^0$.  




