% !TEX root = main_min_disc_dist.tex

We have presented a novel minimum discounted reward HJ formulation for approximating reachable sets. The main advantage of this new formulation over previous work is that the solution can be obtained as the unique fixed point to a contraction mapping. We also showed how other solutions like policy iteration, and multigrid approaches can be used to yield faster convergence. 

The benefits listed so far, are  within the context of the traditional control paradigm, where we have or assume a fixed model of the system under consideration. However, as learning and data-driven approaches become more powerful and pervasive, perhaps the greatest contribution of this work is that it can lie somewhere in between traditional control and model-free reinforcement learning. The approach is certainly model-based, but because of its agnosticism to initialization it also has the flexibility to incorporate data and build towards solutions iteratively. We foreshadowed how this can be done with temporal difference learning, and in the future we plan on exploring how RL algorithms can be used with this formulation to approximate reachable sets for systems with unknown models or that are high-dimensional. 