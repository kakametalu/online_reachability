% !TEX root = main_min_disc_dist.tex
In this section we present methods that may yield much faster convergence than value iteration. These approaches have been extensively applied to the SDR setting, and we now apply them to the MDR problem. 

\subsection{Policy Iteration}

In the one player setting (control only), if the backup operator is a contraction mapping the solution can also be obtained via \emph{policy iteration}. First define the \emph{policy backup operator} $B^{\pi}[\cdot]: \RR^{n_G} \rightarrow \RR^{n_G}$, 
%
\begin{equation} \label{eq:backup_policy}
B^{\pi}[\vec{A}] = \min\left\{ \vec{h}, \gamma \Phi_{\pi} \vec{A} \right \} .
\end{equation}%
\noindent This operator is a contraction mapping. The policy iteration algorithm is then given by
%
\begin{subequations}\label{eq:pi}
\begin{align}
&\vec{U}^{\pi^k} = B^{\pi^k}[\vec{U}^{\pi^k}] \label{eq:pi_a} ,\\
&\pi^{k+1} = \arg\underset{\pi}{\max}B^{\pi}[\vec{U}^{\pi^k}] ,\\ 
&\vec{U} = \lim_{k\rightarrow \infty} \vec{U}^{\pi^k}.
\end{align}
\end{subequations}

Note that ${\vec{U}^{\pi^k}}$, the fixed point of \eqref{eq:pi_a}, is obtained through value iteration with the policy backup. Finding the fixed point of the policy backup is computationally less intensive than the other backup operators presented thus far, since no optimization is performed over policies. The policy iteration algorithm only optimizes over policies when switching policies.  

In practice policy iteration is typically recommended over value iteration because policies can converge faster than values resulting in faster convergence of the algorithm \cite{Russell2003}. A more detailed analysis of policy iteration (as it pertains to SDR) can be found in \cite{Bokanowski2009,Howard1964, Puterman1979}. 

We now prove that policy iteration converges for the MDR problem.
%
\begin{proposition}
Assuming a finite control set ${\U=\{u_i\}_{i=1}^{n_\U}}$, the policy iteration algorithm converges to the vectorized value function obtained from \eqref{eq:val_iter_backup} without the disturbance.
\end{proposition}%
\noindent \begin{proof}
It's sufficient to show that sequence ${\{ \vec{U}^{\pi^{k}}\}}_k$ is nondecreasing, i.e. $\vec{U}^{\pi^{k+1}} \geq \vec{U}^{\pi^k}$ $\forall k$. Since the number of policies is finite the nondecreasing criterion implies that the sequence of vectors will converge. Also note that $\underset{\pi}{\max}\text{ }B^{\pi}[\cdot] = B[\cdot]$ as defined in \eqref{eq:val_iter_backup} without the disturbance, so the sequence converges to the vectorized value function.

Consider two sequences ${\vec{X}^{i+1}=\min\big\{\vec{U}^{\pi^k}, \gamma \Phi_{\pi^{k+1}}\vec{X}^i \big\}}$ and ${\vec{Y}^{i+1}=\min\big\{\vec{h}, \gamma \Phi_{\pi^{k+1}} \vec{Y}^i \big\}}$, with ${\vec{X}^0=\vec{Y}^0=\vec{U}^{\pi^k}}$.
Since ${\vec{h}\geq \vec{U}^{\pi^k}}$, by \eqref{eq:pi_a}, we have ${\vec{Y}^i \geq \vec{X}^i,\forall i \geq 0}$.
Next, we note that ${\vec{X}^{1}=\min\big\{\vec{U}^{\pi^k},\gamma \Phi_{\pi^{k+1}}\vec{U}^{\pi^k}\big\}}$ ${=\min\big\{\vec{h}, \gamma \Phi_{\pi^{k}}\vec{U}^{\pi^k},\gamma \Phi_{\pi^{k+1}}\vec{U}^{\pi^k}\big\}}$.
Furthermore, the third term in the $\min\{\,\}$ is greater than the second, so $\vec{X}^{1}=\min\big\{\vec{h}, \gamma \Phi_{\pi^{k}}\vec{U}^{\pi^k} \big\} =  \vec{U}^{\pi^k}$, thus ${\vec{X}^{i} =  \vec{U}^{\pi^k},\forall i \geq 0}$.
Lastly, $\lim_{i\rightarrow \infty}\vec{Y}^i= \vec{U}^{\pi^{k+1}}$, which is the fixed-point of the contraction mapping that generates the sequence.
Bringing everything together we have $\vec{U}^{\pi^{k+1}} = \lim_{i\rightarrow \infty}\vec{Y}^i \geq   \lim_{i\rightarrow \infty}\vec{X}^i = \vec{U}^{\pi^{k}}.$
\end{proof}



\subsection{Multigrid Approach}

The accuracy of the approximation scheme depends on the fineness of the discretization. Finer grids have lower approximation error, but at the cost of increased computational effort. For a desired level of accuracy the number of grid points (and thus computation) necessary grows exponentially with the state space dimension, which is the well-known \emph{curse of dimensionality}. 

One possible way to manage this trade-off between computation and accuracy is a multigrid approach. The idea is to first solve for the approximation on a coarse grid (e.g. grid spacing $2\Delta x_j$) and then use the final solution to initialize either value iteration or policy iteration on a finer grid. The procedure can also be stacked, i.e. given $m$ grids of increasing fineness we can produce approximations of increasing accuracy by using each as an initialization for the subsequent grid. This is only possible because contraction mappings allow great flexibility in the initialization, and yield faster convergence for good initializations. Due to the curse of dimensionality obtaining a good approximation is exponentially cheaper on the coarse grid. 

Multigrid approaches have been applied extensively for SDR problems, where empirical and theoretical improvements have been shown \cite{Alla2015, Chow1991}. We compare multigrid approaches to value iteration and policy iteration in Section \ref{sec:sim}.
