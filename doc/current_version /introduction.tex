% !TEX root = main_min_disc_dist.tex

Computing reachable sets have been a popular approach for verifying and validating the behavior of dynamical systems. In the reachability problem, one specifies a target set in the state space and then aims to find the set of initial states admitting trajectories that can eventually enter the target within some specified time horizon. The target may represent a region we desire to drive the system to or keep the system away from, which is the case when safety is a concern. Due to its generality, Hamilton-Jacobi (HJ) reachability analysis, which casts the problem in the optimal control framework, has seen broad usage in many safety-critical applications including emergency landing of unmanned aerial vehicles (UAVs) \cite{Ding2016}, vehicle platooning \cite{Chen2015a}, safe learning \cite{Akametalu2014, Gillula2012}, collision-avoidance \cite{Hoffmann2008, Mitchell2005}, and many others \cite{Ding2011a, Huang2011}.

The standard HJ formulation solves a \emph{minimum reward} (MR) optimal control problem, to be presented later, by computing the viscosity solution of a particular time-dependent HJ partial differential equation (PDE) \cite{Mitchell2005}. Like most optimal control problems the solution is solved approximately on a grid via the application of dynamic programming (DP). One downside of MR optimal control problems is that the resulting DP algorithm does not produce a contraction mapping. When the solution is the fixed point of a contraction mapping one can initialize the algorithm with good guesses to obtain faster convergence. Infinite horizon \emph{sum of discounted reward} (SDR) problems yield contraction mappings, and ultimately allow for more efficient solution methods.    

% There are two common HJ formulations for computing reachable sets, one  obtains the \emph{minimum distance} (to target) function by solving a time-dependent HJ partial differential equation (PDE) \cite{Mitchell2005} and the other obtains the \emph{minimum time to reach} (target) function by solving a time-independent HJ PDE \cite{Bardi1999, Falcone1994}. Both PDEs are solved approximately through the application of dynamic programming (DP). Numerically, the minimum distance formulation is preferred because its solution is continuous, in contrast with the minimum time to reach formulation which can have discontiuities in the solution making it difficult to characterize the reachable sets. However, the DP algorithm associated with the time-independent formulation yields a contraction mapping, which can leverage good solution estimates to more quickly converge to the correct solution. allow the probelm to be solved more efficiently when a good guess to the solution is provided. The time-dependent formulation cannot leverage such knowledge.    
 
In this paper we propose a \emph{minimum discounted reward} (MDR) formulation for computing tight over and under approximations of infinite horizon reachable sets. This new formulation yields a contraction mapping and allows for more efficient solution methods like policy iteration or multigrid approaches, which until now have been reserved for SDR problems. This new formulation also provides a way forward for learning reachable sets when dynamics are unknown or difficult to model, a topic that has garnered some attention recently \cite{Akametalu2014,Akametalu2015,Allen2014,Djeridane2006,Gillula2012a,Royo2016}. This is analogous to Reinforcement Learning (RL), which attempts to solve the infinite horizon SDR problem when the system model is unknown. Most RL algorithms are based on finding a fixed-point to a contraction mapping. We are hopeful that the techniques developed in RL, like temporal differencing \cite{Sutton1988}, can also be applied in this setting and facilitate research in learning reachable sets.

 % ..This that both yields a solution that is continuous, and a DP algorithm that acts as a contraction mapping with a unique fixed point. Implicit surface representations of over and under approximations of the reachable set can be obtained from the solution. The error in the approximations can be controlled by the amount of discounting introduced, which also impacts the convergence properties of the DP algorithm. 

The paper is organized as follows in Section \ref{sec:back} we briefly go over the MR formulation for HJ reachability analysis, and SDR problems. In Section \ref{sec:mdr} we describe the minimum discounted  reward formulation, and prove some relevant results. In Section \ref{sec:conv} we explore more efficient soluiton methods within the context of the new formulation. Section \ref{sec:learn} draws inspiration from RL and presents some preliminary ideas on how the formulation can be used for learning reachable sets. Section \ref{sec:sim} contains examples demonstrating the ideas developed throughout the paper, and we conclude the paper in Section \ref{sec:end}.


% Computing reachable sets have been a popular approach for verifying and validating the behavior of dynamical systems. In most applications, practitioners specify a target set in the state space and then aim to find the set of initial states admitting trajectories that can eventually enter the target within some specified time horizon, i.e. the \emph{backward reachable set}. The reachable set can be used to provide guarantees on the liveness of the system if the goal is to enter the target set, e.g. finding the basin of attraction for a reference point. It can also be used for safety applications, where the target set represents unsafe states and the reachable set thus represents states that are potentially unsafe, e.g. collision avoidance.

% One of the most general approaches for computing reachable sets is through Hamilton-Jacobi (HJ) reachability analysis which has its theoretical underpinnings in optimal control and differential games. The approach can handle non-linear dynamics, and only requires that the target set be closed.  Other methods restrict the target to either hyperplanes or ellipsoids \cite{Frehse2011, Greenstreet1998, Kurzhanski2000}. Due to its generality HJ reachability analysis has been used in a number of safety-critical applications including emergency landing of unmanned aerial vehicles (UAVs) \cite{Ding2016}, vehicle platooning \cite{Chen2015a}, safe learning \cite{Akametalu2014, Gillula2012}, collision-avoidance \cite{Hoffmann2008, Mitchell2005}, and many others \cite{Ding2011a, Huang2011}. 

% The reachable sets can be constructed either through a \emph{minimum distance} (also referred to as time-dependent) or \emph{minimum time to reach} (also referred to as time-independent) HJ formulation. In the minimum distance formulation the viscosity solution to a particular time-dependent HJ partial differential equation (PDE) is shown to be an implicit surface representation of the backwards reachable set \cite{Mitchell2005}. This same viscosity solution is also obtained by solving a particular time-dependent HJ variational inequality (VI) \cite{Barron1989, Barron1990}. The solution represents the \emph{minimum distance function}, i.e. for any given initial state and time horizon the minimum distance to the target set the system will realize along an optimal trajectory. \footnote{The term distance here is user defined and quite general. The only requirement is that the distance function is continuous, negative inside the target set, and nonnegative otherwise. Optimality of the trajectory depends on whether the objective is to enter the target or avoid the target.} The minimum time to reach formulation obtains the minimum time to reach function as a solution to a time-independent HJI PDE  \cite{Bardi1999, Falcone1994}. The reachable sets are then just super-zero level sets of the time to reach function. 

% The authors of \cite{Mitchell2005} state some advantages for using the minimum distance formulation. The most important advantage is that the minimum distance function is continuous, and thus numerically easier to handle than the minimum time to reach function which can have discontinuities. Finally, the time to reach formulation is not well-defined outside the reachable set, thus it provides no information on constructing control laws to avoid the reachable set in safety applications. The minimum distance formulation does not have this limitation.

% This paper is focused on the infinite horizon reachable set, which is relevant for scenarios where the system may be in operation for long periods of time, e.g. reinforcement learning or vehicle platooning. In the infinite horizon context the minimum time to reach formulation has a key advantage over the minimum distance formulation. In both approaches the solutions are approximated on a grid, and iteratively solved until convergence via dynamic programming (DP). The minimum distance formulation is only guaranteed to converge to the correct result if the DP algorithm is initialized with the distance function. On the other hand, the minimum time to reach formulation is agnostic to the initialization. A key insight here is if a good approximation to the solution is used as an initialization, then the DP algorithm will have faster convergence. This is the underlying idea behind policy iteration and multi grid approaches, which both try to obtain good approximations cheaply and often times perform better than the standard DP algorithm.

% Since the minimum time to reach function is unbounded (for states that never reach the target), it is actually solved as an infinite horizon sum of discounted rewards problem through the Kruzkhov transform \cite{Falcone2014}. Discounting induces a DP algorithm that acts as a contraction mapping with a unique fixed point.

% In this paper we propose a discounted minimum distance formulation that both yields a solution that is continuous, and a DP algorithm that acts as a contraction mapping with a unique fixed point. Implicit surface representations of over and under approximations of the reachable set can be obtained from the solution. The error in the approximations can be controlled by the amount of discounting introduced, which also impacts the convergence properties of the DP algorithm. 

% The paper is organized as follows in Section \textcolor{red}{Blah} we briefly go over the minimum distance formulation for HJI reachability analysis, and sum of discounted reward problems. In Section \textcolor{red}{Blah} we describe the discounted minimum distance formulation, and prove some relevant results (continuity of the solution, DP algorithm is a contraction mapping, etc.). In Section \textcolor{red}{Blah} we present some ideas for producing good approximations for improved convergence, which include policy iteration and multigrid approaches. Section \textcolor{red}{Blah} draws inspiration from Reinforcement Learning and presents some preliminary ideas on how the formulation can be used for learning the reachable set when the system model is not known or cannot be written down explicitly. Section \textcolor{red}{Blah} contains examples demonstrating the ideas developed throughout the paper, and finally we close with our conclusion in Section \textcolor{red}{Blah}.
