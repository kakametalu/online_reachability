% !TEX root = main_min_disc_dist.tex

Computing reachable sets has been a popular approach for verifying and validating the behavior of dynamical systems. In the reachability problem, one specifies a target set in the state space and then aims to find the set of initial states admitting trajectories that can eventually enter the target within some specified time horizon. The target may represent a region we desire to drive the system to or keep the system away from. Due to its generality, Hamilton-Jacobi (HJ) reachability analysis, which casts the problem in the optimal control framework, has seen broad usage in many safety-critical applications \cite{Akametalu2014, Ding2016, Chen2015a}.

The standard HJ formulation for reachability solves a \emph{minimum reward} (MR) optimal control problem, by computing the viscosity solution of a particular time-dependent HJ partial differential equation (PDE) \cite{Mitchell2005}. Like most optimal control problems the solution is solved approximately on a grid via value iteration, which recursively applies a \emph{backup operator} until convergence. One downside of MR optimal control problems is that the backup operator in this setting is not a contraction mapping, thus a specific initialization of value iteration is required for convergence to the correct solution. A contraction mapping allows for arbitrary initialization, and with a good initialization convergence can be accelerated. This is particularly useful when computing reachable sets for multiple systems with similar dynamics allowing for the solution of one problem to be used as an initialization for the other.

On the other hand, infinite horizon \emph{sum of discounted reward} (SDR) problems yield backup operators that are contraction mappings \cite{Bertsekas1995}. This allows for more efficient solution methods, like policy iteration \cite{Howard1964, Puterman1979} and multigrid approaches \cite{Alla2015, Chow1991}.   
 
Drawing inspiration from SDR problems, we propose a \emph{minimum discounted reward} (MDR) formulation for computing tight over- and under-approximations of infinite horizon reachable sets. This new formulation yields a contraction mapping, making it possible to extend policy iteration and multigrid approaches to this setting. In addition, it also provides a way forward for learning reachable sets when dynamics are unknown or difficult to model, a topic that has garnered some attention recently \cite{Akametalu2015,Djeridane2006}. This draws greatly from Reinforcement Learning (RL), which attempts to solve the infinite horizon SDR problem when the system model is unknown. Many RL algorithms boil down to finding the fixed-point of the backup operator associated with SDR. The techniques developed in RL, like temporal differencing \cite{Sutton1988} and Q-learning \cite{Watkins1992}, can naturally be extended to do the same in the MDR setting, thus facilitating research in learning reachable sets.

The paper is organized as follows in Section \ref{sec:back} we briefly go over the MR formulation for HJ reachability analysis, and SDR problems. In Section \ref{sec:mdr} we describe the minimum discounted  reward formulation, and prove some relevant results. In Section \ref{sec:conv} we explore policy iteration and multigrid approaches within the context of the MDR. Section \ref{sec:learn}draws inspiration from RL and presents some preliminary ideas on how the formulation can be used for learning reachable sets. Section \ref{sec:sim} contains examples demonstrating the ideas developed throughout the paper, and we conclude the paper in Section \ref{sec:end}.

\subsection*{Terminology}

The term \emph{optimal control} is typically reserved for the one-player setting, and \emph{differential game} is used when considering problems with two or more players. Here, both the one-player and two-player settings are considered, and we opt to use the terminology from optimal control for consistency. For example \emph{optimal control} will be used to refer to both settings, and we use terms like \emph{reward} instead of \emph{payoff} throughout.

% Computing reachable sets have been a popular approach for verifying and validating the behavior of dynamical systems. In most applications, practitioners specify a target set in the state space and then aim to find the set of initial states admitting trajectories that can eventually enter the target within some specified time horizon, i.e. the \emph{backward reachable set}. The reachable set can be used to provide guarantees on the liveness of the system if the goal is to enter the target set, e.g. finding the basin of attraction for a reference point. It can also be used for safety applications, where the target set represents unsafe states and the reachable set thus represents states that are potentially unsafe, e.g. collision avoidance.

% One of the most general approaches for computing reachable sets is through Hamilton-Jacobi (HJ) reachability analysis which has its theoretical underpinnings in optimal control and differential games. The approach can handle non-linear dynamics, and only requires that the target set be closed.  Other methods restrict the target to either hyperplanes or ellipsoids \cite{Frehse2011, Greenstreet1998, Kurzhanski2000}. Due to its generality HJ reachability analysis has been used in a number of safety-critical applications including emergency landing of unmanned aerial vehicles (UAVs) \cite{Ding2016}, vehicle platooning \cite{Chen2015a}, safe learning \cite{Akametalu2014, Gillula2012}, collision-avoidance \cite{Hoffmann2008, Mitchell2005}, and many others \cite{Ding2011a, Huang2011}. 

% The reachable sets can be constructed either through a \emph{minimum distance} (also referred to as time-dependent) or \emph{minimum time to reach} (also referred to as time-independent) HJ formulation. In the minimum distance formulation the viscosity solution to a particular time-dependent HJ partial differential equation (PDE) is shown to be an implicit surface representation of the backwards reachable set \cite{Mitchell2005}. This same viscosity solution is also obtained by solving a particular time-dependent HJ variational inequality (VI) \cite{Barron1989, Barron1990}. The solution represents the \emph{minimum distance function}, i.e. for any given initial state and time horizon the minimum distance to the target set the system will realize along an optimal trajectory. \footnote{The term distance here is user defined and quite general. The only requirement is that the distance function is continuous, negative inside the target set, and nonnegative otherwise. Optimality of the trajectory depends on whether the objective is to enter the target or avoid the target.} The minimum time to reach formulation obtains the minimum time to reach function as a solution to a time-independent HJI PDE  \cite{Bardi1999, Falcone1994}. The reachable sets are then just super-zero level sets of the time to reach function. 

% The authors of \cite{Mitchell2005} state some advantages for using the minimum distance formulation. The most important advantage is that the minimum distance function is continuous, and thus numerically easier to handle than the minimum time to reach function which can have discontinuities. Finally, the time to reach formulation is not well-defined outside the reachable set, thus it provides no information on constructing control laws to avoid the reachable set in safety applications. The minimum distance formulation does not have this limitation.

% This paper is focused on the infinite horizon reachable set, which is relevant for scenarios where the system may be in operation for long periods of time, e.g. reinforcement learning or vehicle platooning. In the infinite horizon context the minimum time to reach formulation has a key advantage over the minimum distance formulation. In both approaches the solutions are approximated on a grid, and iteratively solved until convergence via dynamic programming (DP). The minimum distance formulation is only guaranteed to converge to the correct result if the DP algorithm is initialized with the distance function. On the other hand, the minimum time to reach formulation is agnostic to the initialization. A key insight here is if a good approximation to the solution is used as an initialization, then the DP algorithm will have faster convergence. This is the underlying idea behind policy iteration and multi grid approaches, which both try to obtain good approximations cheaply and often times perform better than the standard DP algorithm.

% Since the minimum time to reach function is unbounded (for states that never reach the target), it is actually solved as an infinite horizon sum of discounted rewards problem through the Kruzkhov transform \cite{Falcone2014}. Discounting induces a DP algorithm that acts as a contraction mapping with a unique fixed point.

% In this paper we propose a discounted minimum distance formulation that both yields a solution that is continuous, and a DP algorithm that acts as a contraction mapping with a unique fixed point. Implicit surface representations of over and under approximations of the reachable set can be obtained from the solution. The error in the approximations can be controlled by the amount of discounting introduced, which also impacts the convergence properties of the DP algorithm. 

% The paper is organized as follows in Section \textcolor{red}{Blah} we briefly go over the minimum distance formulation for HJI reachability analysis, and sum of discounted reward problems. In Section \textcolor{red}{Blah} we describe the discounted minimum distance formulation, and prove some relevant results (continuity of the solution, DP algorithm is a contraction mapping, etc.). In Section \textcolor{red}{Blah} we present some ideas for producing good approximations for improved convergence, which include policy iteration and multigrid approaches. Section \textcolor{red}{Blah} draws inspiration from Reinforcement Learning and presents some preliminary ideas on how the formulation can be used for learning the reachable set when the system model is not known or cannot be written down explicitly. Section \textcolor{red}{Blah} contains examples demonstrating the ideas developed throughout the paper, and finally we close with our conclusion in Section \textcolor{red}{Blah}.
